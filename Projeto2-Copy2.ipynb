{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Automático de Sentimento\n",
    "\n",
    "Você foi contratado por uma empresa parar analisar como os clientes estão reagindo a um determinado produto no Twitter. A empresa deseja que você crie um programa que irá analisar as mensagens disponíveis e classificará como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mereçam destaque, disparem um foco de atenção da área de marketing.<br /><br />\n",
    "Como aluno de Ciência dos Dados, você lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que é largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conteúdo.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, você precisa implementar uma versão do classificador que \"aprende\" o que é relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Após validado, o seu protótipo poderá também capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informações do Projeto\n",
    "\n",
    "Prazo: 13/Set até às 23:59.<br />\n",
    "Grupo: 1 ou 2 pessoas.<br /><br />\n",
    "Entregáveis via GitHub: \n",
    "* Arquivo notebook com o código do classificador, seguindo as orientações abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**NÃO disponibilizar o arquivo com os *access keys/tokens* do Twitter.**\n",
    "\n",
    "\n",
    "### Check 3: \n",
    "\n",
    "Até o dia 06 de Setembro às 23:59, o notebook e o xlsx devem estar no Github com as seguintes evidências: \n",
    "    * Conta no twitter criada.\n",
    "    * Produto escolhido.\n",
    "    * Arquivo Excel contendo a base de treinamento e teste já classificado.\n",
    "    \n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "http://docs.tweepy.org/en/v3.5.0/index.html<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Preparando o ambiente\n",
    "\n",
    "Instalando a biblioteca *tweepy* para realizar a conexão com o Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as Bibliotecas que serão utilizadas. Esteja livre para adicionar outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "Para realizar a captura dos dados é necessário ter uma conta cadastrada no twitter:\n",
    "\n",
    "* Conta: ***[Preencha aqui o id da sua conta. Ex: @fulano ]***\n",
    "\n",
    "\n",
    "1. Caso ainda não tenha uma: https://twitter.com/signup\n",
    "1. Depois é necessário registrar um app para usar a biblioteca: https://apps.twitter.com/\n",
    "1. Dentro do registro do App, na aba Keys and Access Tokens, anotar os seguintes campos:\n",
    "    1. Consumer Key (API Key)\n",
    "    1. Consumer Secret (API Secret)\n",
    "1. Mais abaixo, gere um Token e anote também:\n",
    "    1. Access Token\n",
    "    1. Access Token Secret\n",
    "    \n",
    "1. Preencha os valores no arquivo \"auth.pass\"\n",
    "\n",
    "**ATENÇÃO**: Nunca divulgue os dados desse arquivo online (GitHub, etc). Ele contém as chaves necessárias para realizar as operações no twitter de forma automática e portanto é equivalente a ser \"hackeado\". De posse desses dados, pessoas mal intencionadas podem fazer todas as operações manuais (tweetar, seguir, bloquear/desbloquear, listar os seguidores, etc). Para efeito do projeto, esse arquivo não precisa ser entregue!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @dudabicalhocd\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Coletando Dados\n",
    "\n",
    "Agora vamos coletar os dados. Tenha em mente que dependendo do produto escolhido, não haverá uma quantidade significativa de mensagens, ou ainda poder haver muitos retweets.<br /><br /> \n",
    "Configurando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Bradesco'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "TweepError",
     "evalue": "Twitter error response: status code = 400",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-202-910742af87dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmsgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproduto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mmsgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\duda\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\duda\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[1;31m# Reached end of current page, get the next page...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\duda\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__self__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\duda\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;31m# Set pagination mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\duda\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[1;31m# Parse the response payload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTweepError\u001b[0m: Twitter error response: status code = 400"
     ]
    }
   ],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "    msgs.append(msg.text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificando as Mensagens\n",
    "\n",
    "Agora você deve abrir o arquivo Excel com as mensagens capturadas e classificar na Coluna B se a mensagem é relevante ou não.<br /> \n",
    "Não se esqueça de colocar um nome para a coluna na célula **B1**.<br /><br />\n",
    "Fazer o mesmo na planilha de Controle.\n",
    "\n",
    "___\n",
    "## Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. Escreva o seu código abaixo:\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.<br />\n",
    "* Corrigir separação de espaços entre palavras e/ou emojis.\n",
    "* Propor outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duda\\Documents\\cdd\\Projeto2-CD\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "dados = pd.read_excel('Bradesco.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "#pegando os dados interessantes para nos, Relevantes\n",
    "dr= dados.Respostas[dados.Relevancia=='r']\n",
    "print(len(dr))\n",
    "probder= 91/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "#Irrelevantes\n",
    "di= dados.Respostas[dados.Relevancia=='i']\n",
    "print(len(di))\n",
    "probdei=209/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\n"
     ]
    }
   ],
   "source": [
    "dicior={} #dicionario que contem a palavra e a quantidade de vezes que ela aparece.\n",
    "palavras=[]\n",
    "Desconsiderar=0 #palavras que podemos desconsiderar, como sites\n",
    "# limpando as palavras e as adicionando a um dicionario\n",
    "for x in dr:\n",
    "    nomes=x.split()\n",
    "    for y in nomes:\n",
    "        p2=[]\n",
    "        if y[0]=='h' and y[1]=='t':\n",
    "            y='.'\n",
    "            Desconsiderar+=1\n",
    "        if y[0]== '@':\n",
    "            y='.'\n",
    "            Desconsiderar+=1\n",
    "        \n",
    "        for i in y : \n",
    "            if i!= \":\" and i!=\"'\" and i!='\"' and i!=' ' and i!= \",\" and i!= \".\" and i!='?'and i!='!' and i!='(' and i!=')' :\n",
    "                p2.append(i)\n",
    "        a=\"\".join(p2)\n",
    "        palavras.append(a)\n",
    "        if x in dicior:\n",
    "            dicior[y]+=1\n",
    "        else:\n",
    "            dicior[y]=1\n",
    "        \n",
    "            \n",
    "#print(dicior)\n",
    "#print(palavras)\n",
    "\n",
    "#total de palavras relevantes\n",
    "tr=sum(dicior.values())-Desconsiderar\n",
    "print(tr)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2687\n"
     ]
    }
   ],
   "source": [
    "dicioi={} #dicionario que contem a palavra e a quantidade de vezes que ela aparece.\n",
    "palavrasi=[]\n",
    "Desconsiderari=0 #palavras que podemos desconsiderar, como sites\n",
    "# limpando as palavras e as adicionando a um dicionario\n",
    "for x in di:\n",
    "    nomes=x.split()\n",
    "    for y in nomes:\n",
    "        p2=[]\n",
    "        if y[0]=='h' and y[1]=='t':\n",
    "            y='.'\n",
    "            Desconsiderari+=1\n",
    "        if y[0]== '@':\n",
    "            y='.'\n",
    "            Desconsiderari+=1  \n",
    "        for i in y:\n",
    "            if i!= \":\" and i!=\"'\" and i!='\"' and i!=' ' and i!= \",\" and i!= \".\" and i!='?'and i!='!' and i!='(' and i!=')' and i!='…' :\n",
    "                p2.append(i)\n",
    "        a=\"\".join(p2)\n",
    "        palavrasi.append(a)\n",
    "        if y in dicioi:\n",
    "            dicioi[y]+=1\n",
    "        else:\n",
    "            dicioi[y]=1\n",
    "\n",
    "#print(dicioi)\n",
    "#print(palavrasi)\n",
    "\n",
    "#total de palavras irrelevantes\n",
    "ti=sum(dicioi.values())-Desconsiderari\n",
    "print(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3930\n"
     ]
    }
   ],
   "source": [
    "#total de palavras existentes nos tweets\n",
    "somar=len(dicioi)\n",
    "for i in dicior:\n",
    "    if i in dicioi:\n",
    "        soma+=0\n",
    "    else:\n",
    "        soma+=1\n",
    "t=soma \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.142944177249135e-11\n"
     ]
    }
   ],
   "source": [
    "#probabilidade da frase ser relevante\n",
    "\n",
    "frase=\"bradesco muito ruim\" #frase de exemplo\n",
    "def probabr(frase,dicior,tr,t):\n",
    "    nomes=frase.split()\n",
    "    pfr=1\n",
    "    for i in nomes:\n",
    "        if i in dicior and  i!= \"bradesco\" :\n",
    "            pa=(dicior[i]+1)/(tr+t)\n",
    "            pfr*=pa\n",
    "        else:\n",
    "            pa=1/(tr+t)\n",
    "            pfr*=pa\n",
    "    return pfr\n",
    "    \n",
    "print(probabr(frase,dicior,tr,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.903138577258877e-12\n"
     ]
    }
   ],
   "source": [
    "#probabilidade da frase ser irrelevante\n",
    "\n",
    "frase=\" bradesco muito ruim\"\n",
    "def probabi(frase,dicioi,ti,t):\n",
    "    nomes=frase.split()\n",
    "    pfi=1\n",
    "    for i in nomes:\n",
    "        if i in dicioi and i != \"bradesco\":\n",
    "            pa= (dicioi[i]+1)/(ti+t)\n",
    "            pfi*=pa\n",
    "        else:\n",
    "            pa=1/(ti+t)\n",
    "            pfi*=pa\n",
    "    return pfi\n",
    "print(probabi(frase,dicioi,ti,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "É relevante\n"
     ]
    }
   ],
   "source": [
    "#comparando qual probabilidade é maior, para vermos se a frase é ou não releante\n",
    "if probabi(frase,dicioi,ti,t)<probabr(frase,dicior,tr,t):\n",
    "    print('É relevante')\n",
    "else:\n",
    "    print('É irrelevante')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Você deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n",
    "Opcionalmente:\n",
    "* Criar categorias intermediárias de relevância baseado na diferença de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duda\\Documents\\cdd\\Projeto2-CD\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "dados2 = pd.read_excel('Bradesco.xlsx', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pegando os dados para a analise\n",
    "dados2.head()\n",
    "d2= dados2.Tweets\n",
    "dad2=dados2.Relevancia\n",
    "#print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def probabrt(frase,dicior,tr,t):\n",
    "    pfr=1\n",
    "    for i in frase:\n",
    "        if i in dicior and i!= \"bradesco\":\n",
    "            pa=(dicior[i]+1)/(tr+t)\n",
    "            pfr*=pa\n",
    "        else:\n",
    "            pa=1/(tr+t)\n",
    "            pfr*=pa\n",
    "    return pfr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabit(frase,dicioi,ti,t):\n",
    "    pfi=1\n",
    "    for i in frase:\n",
    "        if i in dicioi and i!= \"bradesco\":\n",
    "            pa= (dicioi[i]+1)/(ti+t)\n",
    "            pfi*=pa\n",
    "        else:\n",
    "            pa=1/(ti+t)\n",
    "            pfi*=pa\n",
    "    return pfi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste=[]\n",
    "palavrasi=[]\n",
    "nomes2=[]\n",
    "frases=[]\n",
    "\n",
    "\n",
    "for x in d2:\n",
    "    nomes=x.split()\n",
    "    k=0\n",
    "    nomes2=[]\n",
    "    for y in nomes: \n",
    "        \n",
    "        \n",
    "        p2=[] \n",
    "        for i in y:\n",
    "            if i!= \":\" and i!=\"'\" and i!='\"' and i!=' ' and i!= \",\" and i!= \".\" and i!='?'and i!='!' and i!='(' and i!=')' and i!='…' :\n",
    "                p2.append(i)\n",
    "    \n",
    "        a=\"\".join(p2)\n",
    "        \n",
    "        \n",
    "        if y[0]!= \"@\" :\n",
    "            nomes2.append(nomes[k])\n",
    "        k+=1\n",
    "    frases.append(nomes2)\n",
    "#print(frases)\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "7\n",
      "1\n",
      "44\n",
      "-----\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "Relevancia2=[]\n",
    "ii=0\n",
    "ir=0\n",
    "rr=0\n",
    "ri=0\n",
    "\n",
    "#definindo a relevancia de cada frase da base Teste calculada pelo nosso código\n",
    "for i in frases:\n",
    "   \n",
    "    pi= probabit(i,dicioi,ti,t)\n",
    "    pr= probabrt(i,dicior,tr,t)\n",
    "    if pi<pr:\n",
    "        Relevancia2.append('r')\n",
    "    else:\n",
    "        \n",
    "        Relevancia2.append('i')\n",
    "        \n",
    "#fazendo uma lista de relevancia marcada por nós da base Teste\n",
    "Relevancia=[]\n",
    "for i in dad2:\n",
    "    Relevancia.append(i)\n",
    "\n",
    "#comparando os resultados    \n",
    "for x in range(len(Relevancia2)):\n",
    "    if Relevancia[x]=='i' and Relevancia2[x]=='i':\n",
    "        ii+=1\n",
    "    if Relevancia[x]=='i' and Relevancia2[x]=='r':\n",
    "        ir+=1\n",
    "    if Relevancia[x]=='r' and Relevancia2[x]=='r':\n",
    "        rr+=1\n",
    "    if Relevancia[x]=='r' and Relevancia2[x]=='i':\n",
    "        ri+=1\n",
    "            \n",
    "\n",
    "print(ii)\n",
    "print(ir)\n",
    "print(rr)\n",
    "print(ri)\n",
    "\n",
    "print('-----')\n",
    "print(ii+ir+rr+ri)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pri =\n",
      "97.77777777777777\n",
      "prr =\n",
      "2.2222222222222223\n",
      "pii =\n",
      "95.48387096774194\n",
      "pir =\n",
      "4.516129032258064\n"
     ]
    }
   ],
   "source": [
    "#Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "pri=ri/45 *100\n",
    "print('pri =')\n",
    "print(pri)\n",
    "#Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "prr=rr/45*100\n",
    "print('prr =')\n",
    "print(prr)\n",
    "#Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "pii=ii/155*100\n",
    "print('pii =')\n",
    "print(pii)\n",
    "#Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "pir=ir/155*100\n",
    "print('pir =')\n",
    "print(pir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "Escreva sua conclusão aqui.<br />\n",
    "Faça um comparativo qualitativo sobre as medidas obtidas.<br />\n",
    "Explique como são tratadas as mensagens com dupla negação e sarcasmo.<br />\n",
    "Proponha um plano de expansão. Por que eles devem continuar financiando o seu projeto?<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que não posso alimentar minha base de Treinamento automaticamente usando o próprio classificador, aplicado a novos tweets.\n",
    "* Propor diferentes cenários de uso para o classificador Naive-Bayes. Cenários sem intersecção com este projeto.\n",
    "* Sugerir e explicar melhorias reais no classificador com indicações concretas de como implementar (não é preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "Podemos concluir que nosso Classificador pode analisar bem os dados considerados irrelevantes, apresentando uma porcentagem de aproximadamente 88,4% de acertos, mas que sua analise sobre os relevantes não foi tão boa, com apenas 6% de acertos. <br />\n",
    "<br />\n",
    "\n",
    "Podemos perceber que algumas frases consideradas relevantes pelo programa mas não por nós, acabam se tornando relevantes por apresentar palavras mais difíceis de nosso sistema reconhecer pela filtragem que fizemos nas palavras, explicando a ineficácia do programa. Alguns exemplos são:<br />\n",
    "-@robertolasilva @glaysinha @john_silva2 @anacec97 @xarope_bolado @josi_sborges @matheuscecmdm @daabragahttps://t.co/08emukfgyp<br />\n",
    "-@stevenstrogatz @jsellenberg @luisc @bancopaulista @bradesco @mobil @votorantim @citibank @billgates @batinaphttps://t.co/py2txblpse<br />\n",
    "-#timbeta #betaajudabeta #timbetaajudatimbeta #betaajudabeta  #betaquerlab  #timrepin #pinterest (@ bradesco)https://t.co/m0x0tscsy9<br />\n",
    "<br />\n",
    "<br />\n",
    "Já, podemos perceber que as palavras que por nós eram relevantes, mas consideradas irrelevantes pelo sistema, foram consideradas assim pela quantidade de palavras importantes ser grande e o dicionário que usamos como base para as irrelevantes terem maior número de palavras, por grande parte dos tweets da base de Treinamento ser considerada Irrelevante,209 de 300 frases, contando com 2687 palavras irrelevantes, e 91 de 300 frases eram relevantes, com 606 palavras diferentes.  \n",
    "Outro problema são as palvras que apareceram muito mais A palavra \"bradesco\" por exemplo \n",
    "\n",
    "Alguns exeplos de frases relevantes consideradas irrelevantes são:<br />\n",
    "-@bradesco oi isa bom dia. cá estou na agencia do bradesco e apenas 2 cx funcionando em época de pgto de idosos e sehttps://t.co/bbvr6jsccx<br />\n",
    "-@bradesco tô com dificuldade de tirar o dinheiro da minha conta, me ajuda<br />\n",
    "<br />\n",
    "uma Palavra que nao apareceu ou apaereceu mito, por acaso bradesco aparece  mais em irrelevante\n",
    "\n",
    "<br />\n",
    "(não sei o que falar das palavras de sarcasmo!!)<br />\n",
    "<br />\n",
    "Essa analise evidencia  a importância de uma melhora no projeto, contando com mais características para poder distinguir frases relevantes e irrelevantes, e para isso um maior investimento nele. Isso ajuda no desenvolvimento e melhoria do Banco, que consegue analisar diretamente as frases relevantes para seu estudo, e assim poder saber com mais facilidade os problemas enfrentados por seus  usuários.\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
